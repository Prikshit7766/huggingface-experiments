{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    " \n",
    "BASE_URL = \"https://discuss.huggingface.co/\"\n",
    "SAVE_DIR = \"huggingface_forum\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "categories = {\n",
    "    \"Research\": \"https://discuss.huggingface.co/c/research/7\",\\\n",
    "    \"beginners\": \"https://discuss.huggingface.co/c/beginners/5\", \\\n",
    "    \"intermediate\": \"https://discuss.huggingface.co/c/intermediate/6\", \\\n",
    "    \"course\": \"https://discuss.huggingface.co/c/course/20\", \\\n",
    "    \"models\": \"https://discuss.huggingface.co/c/models/13\", \\\n",
    "    \"transformers\": \"https://discuss.huggingface.co/c/transformers/9\", \\\n",
    "    \"datasets\": \"https://discuss.huggingface.co/c/datasets/10\", \\\n",
    "    \"tokenizers\":\"https://discuss.huggingface.co/c/tokenizers/11\", \\\n",
    "    \"accelerate\": \"https://discuss.huggingface.co/c/accelerate/18\", \\\n",
    "    \"autotrain\": \"https://discuss.huggingface.co/c/autotrain/16\", \\\n",
    "    \"hub\": \"https://discuss.huggingface.co/c/hub/23\", \\\n",
    "    \"optimum\": \"https://discuss.huggingface.co/c/optimum/59\", \\\n",
    "    \"gradio\": \"https://discuss.huggingface.co/c/gradio/26\", \\\n",
    "    \"diffusers\": \"https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers/63\", \\\n",
    "    \"inference-endpoints\": \"https://discuss.huggingface.co/c/inference-endpoints/64\", \\\n",
    "    \"sagemaker\": \"https://discuss.huggingface.co/c/sagemaker/17\", \\\n",
    "    \"aws-inferentia-trainium\": \"https://discuss.huggingface.co/c/aws-inferentia-trainium/66\", \\\n",
    "    \"azureml\": \"https://discuss.huggingface.co/c/azureml/68\", \\\n",
    "    \"google-cloud\": \"https://discuss.huggingface.co/c/google-cloud/69\", \\\n",
    "    \"spaces\": \"https://discuss.huggingface.co/c/spaces/24\", \\\n",
    "    \"model-card\": \"https://discuss.huggingface.co/c/model-cards/14\", \\\n",
    "    \"languages-at-hugging-face\": \"https://discuss.huggingface.co/c/languages-at-hugging-face/15\", \\\n",
    "    \"flax-jax-projects\": \"https://discuss.huggingface.co/c/flax-jax-projects/22\", \\\n",
    "    \"community-calls\": \"https://discuss.huggingface.co/c/community-calls/12\", \\\n",
    "    \"show-and-tell\": \"https://discuss.huggingface.co/c/show-and-tell/65\", \\\n",
    "    \"site-feedback\": \"https://discuss.huggingface.co/c/site-feedback/2\", \\\n",
    "}\n",
    "\n",
    "def scrape_topic_initial_post(topic_url):\n",
    "    \"\"\"Fetch the initial post content, date, and any responses from the topic page.\"\"\"\n",
    "    response = requests.get(topic_url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # Capture initial post content and date\n",
    "    first_post = soup.select_one(\".topic-body .post\")\n",
    "    initial_post = first_post.get_text(strip=True) if first_post else \"Initial post not available\"\n",
    "    initial_post_date = first_post.find_previous(\"time\").get(\"datetime\") if first_post and first_post.find_previous(\"time\") else \"No date available\"\n",
    "    \n",
    "    # Capture responses, skipping the first post\n",
    "    responses = []\n",
    "    reply_posts = soup.select(\".topic-body\")[1:]  # Skip the first post in the selection\n",
    "    for post in reply_posts:\n",
    "        date = post.select_one(\"time\").get(\"datetime\") if post.select_one(\"time\") else \"No date available\"\n",
    "        reply_text = post.select_one(\".post\").get_text(strip=True) if post.select_one(\".post\") else \"No reply text available\"\n",
    "        \n",
    "        responses.append({\n",
    "            \"date\": date,\n",
    "            \"reply\": reply_text\n",
    "        })\n",
    "    \n",
    "    return initial_post, initial_post_date, responses\n",
    "\n",
    "def scrape_topics(category_name, category_url):\n",
    "    topics = []\n",
    "    page = 1  # Start with the first page\n",
    "    \n",
    "    while True:\n",
    "        paginated_url = f\"{category_url}?page={page}\"\n",
    "        response = requests.get(paginated_url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Check if there are any topics on the current page\n",
    "        topic_elements = soup.select(\".topic-list-item\")\n",
    "        if not topic_elements:\n",
    "            break  # No more topics, exit the loop\n",
    "        \n",
    "        for topic in topic_elements:\n",
    "            title_element = topic.select_one(\".title.raw-link.raw-topic-link\")\n",
    "            if not title_element:\n",
    "                continue\n",
    "\n",
    "            title = title_element.get_text(strip=True)\n",
    "            link = urljoin(BASE_URL, title_element[\"href\"])\n",
    "            \n",
    "            replies = topic.select_one(\".replies .posts\")\n",
    "            views = topic.select_one(\".views .views\")\n",
    "            num_replies = int(replies.get_text(strip=True)) if replies else 0\n",
    "            \n",
    "            topic_data = {\n",
    "                \"title\": title,\n",
    "                \"link\": link,\n",
    "                \"replies\": num_replies,\n",
    "                \"views\": int(views.get_text(strip=True)) if views else None,\n",
    "            }\n",
    "            \n",
    "            # Retrieve initial post, initial post date, and responses for each topic\n",
    "            initial_post, initial_post_date, responses = scrape_topic_initial_post(link)\n",
    "            topic_data[\"initial_post\"] = initial_post\n",
    "            topic_data[\"initial_post_date\"] = initial_post_date\n",
    "            topic_data[\"responses\"] = responses if num_replies > 0 else []  # Only add responses if replies > 0\n",
    "\n",
    "            topics.append(topic_data)\n",
    "            \n",
    "            # Pause to avoid overloading the server\n",
    "            time.sleep(1)\n",
    "        \n",
    "        print(f\"Page {page} processed for category '{category_name}'.\")\n",
    "        page += 1  # Move to the next page\n",
    "    \n",
    "    # Save to JSON file\n",
    "    json_filename = os.path.join(SAVE_DIR, f\"{category_name}.json\")\n",
    "    with open(json_filename, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(topics, json_file, indent=4)\n",
    "    \n",
    "    print(f\"Saved {len(topics)} topics with initial post and responses for category '{category_name}' to {json_filename}\")\n",
    "\n",
    "# Run the scraping for each category\n",
    "for category_name, category_url in categories.items():\n",
    "    scrape_topics(category_name, category_url)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
