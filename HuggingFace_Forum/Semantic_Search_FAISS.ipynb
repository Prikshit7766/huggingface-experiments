{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face Forum Semantic Search with FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\priks\\anaconda3\\envs\\t\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import faiss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing JSON files\n",
    "DATASET_DIR = \"huggingface_forum\"\n",
    "data_files = {\"train\": os.path.join(DATASET_DIR, \"*.json\")}  # Use glob pattern to load all JSON files\n",
    "\n",
    "# Load the dataset\n",
    "huggingface_forum_dataset = load_dataset(\"json\", data_files=data_files, split=\"train\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_entry(entry):\n",
    "    # Retain only the responses from each entry\n",
    "    simplified_entry = {\n",
    "        \"responses\": [response[\"reply\"] for response in entry.get(\"responses\", [])]\n",
    "    }\n",
    "    return simplified_entry\n",
    "\n",
    "huggingface_forum_dataset = huggingface_forum_dataset.map(simplify_entry)\n",
    "\n",
    "# Specify columns to keep and remove unnecessary ones\n",
    "columns = huggingface_forum_dataset.column_names\n",
    "columns_to_keep = [\"title\", \"link\", \"initial_post\", \"responses\"]\n",
    "columns_to_remove = set(columns_to_keep).symmetric_difference(columns)\n",
    "huggingface_forum_dataset = huggingface_forum_dataset.remove_columns(columns_to_remove)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Formatting for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 40193/40193 [00:00<00:00, 224540.53 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Set dataset to pandas format for easier manipulation\n",
    "huggingface_forum_dataset.set_format(\"pandas\")\n",
    "df = huggingface_forum_dataset[:]\n",
    "\n",
    "# Explode responses for individual analysis\n",
    "res_df = df.explode(\"responses\", ignore_index=True)\n",
    "\n",
    "# Add response length column for filtering\n",
    "res_df[\"response_length\"] = res_df[\"responses\"].apply(lambda x: len(x.split()) if isinstance(x, str) else 0)\n",
    "\n",
    "# Convert back to Hugging Face Dataset format\n",
    "res_df = Dataset.from_pandas(res_df)\n",
    "\n",
    "# Filter responses based on length\n",
    "res_df = res_df.filter(lambda x: x[\"response_length\"] > 15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate Text Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 22988/22988 [00:01<00:00, 15969.45 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def concatenate_text(examples):\n",
    "    return {\n",
    "        \"text\": examples[\"title\"] + \" \\n \" + examples[\"initial_post\"] + \" \\n \" + examples[\"responses\"]\n",
    "    }\n",
    "\n",
    "res_df = res_df.map(concatenate_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Generation with Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:, 0]\n",
    "\n",
    "def get_embeddings(text_list):\n",
    "    encoded_input = tokenizer(text_list, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "    return cls_pooling(model_output)\n",
    "\n",
    "# Generate a sample embedding to check the shape\n",
    "embedding = get_embeddings(res_df[\"text\"][0])\n",
    "print(f\"Embedding shape: {embedding.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Embeddings to the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 22988/22988 [04:40<00:00, 81.82 examples/s] \n"
     ]
    }
   ],
   "source": [
    "# Add embeddings as a new column\n",
    "embeddings_dataset = res_df.map(lambda x: {\"embeddings\": get_embeddings(x[\"text\"]).detach().cpu().numpy()[0]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and add FAISS index directly to the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 258.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index successfully added to the dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings_dataset.add_faiss_index(column=\"embeddings\")\n",
    "print(\"FAISS index successfully added to the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and embed a query for semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_faiss(query, top_k=5):\n",
    "    # Generate the query embedding as a NumPy array\n",
    "    query_embedding = get_embeddings([query]).cpu().detach().numpy()\n",
    "    \n",
    "    # Perform search on FAISS index\n",
    "    scores, samples = embeddings_dataset.get_nearest_examples(\"embeddings\", query_embedding, k=top_k)\n",
    "    \n",
    "    # Display search results\n",
    "    results = []\n",
    "    for i in range(len(samples[\"title\"])):\n",
    "        result = {\n",
    "            \"title\": samples[\"title\"][i],\n",
    "            \"link\": samples[\"link\"][i],\n",
    "            \"initial_post\": samples[\"initial_post\"][i],\n",
    "            \"response\": samples[\"responses\"][i]\n",
    "        }\n",
    "        results.append(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: What model will fit better for Email Parsing and Data Extraction\n",
      "Link: https://discuss.huggingface.co/t/what-model-will-fit-better-for-email-parsing-and-data-extraction/70371\n",
      "Initial Post: Hi,Firstly, Apologies if that is not the right section in the forum… (quite new here)Now, I am working in a new project, well in a new idea to automate a current very manual process:Lots of emails come in which are manually processed by “a human” to extract data from them which are then copied in a tabulate software (excel) or similar.I can clearly identify the attributes/fields I want to extract from the text/emails.I have too the types of “text/emails”The “problem” is to be able to extract that information accurately.Email Example:\"Hello my friends.we have two guys arriving tomorrow 23/01/2024 around 1pm from Madrid, flight ABC123Another one people leaving on sunday same week around 2am to Instanbul, flight CBA321Name of the arrving ones: Jose Mateo Feliz y Ana Triste del Carmen.Name of the leaving Matia NodoyunaRegards\"And basically the output required should be a JSON with a fix and depict attribute list filled (or left empty if not found).I have been playing with Mistral and Llama, they are ok, a little slow (different Q tested and B), but I believe my scope is quite small so with the right “small” model and some training (I have thousand of emails examples), I could get a much faster and accurate model…Any thought?Thanks in advance!\n",
      "Response: Hi I’m working with a similar use case. Curious to know if you have found a successful solution. I’ve tried Bert Squad but it was sub optimal. Gpt3.5 is the best but I’m looking for an open source option.\n",
      "\n",
      "Title: Best free options if you want to train a language model on a small set of private documents?\n",
      "Link: https://discuss.huggingface.co/t/best-free-options-if-you-want-to-train-a-language-model-on-a-small-set-of-private-documents/80157\n",
      "Initial Post: I have about a hundred PDF files, can i download a model that understands English and then feed it the docs, so I could talk to the model about them?\n",
      "Response: I would look at vector databasing with RAG for this.Basically you turn all your docs into Embeddings and store them. You then retrieve documents that are similar to your query. Finally you take an off the shelf model such as Mistral 7B and you give it the relevant documents as context and then ask it questions on the retrieved docs.\n",
      "\n",
      "Title: Suggestions on Invoice Extraction LLMs\n",
      "Link: https://discuss.huggingface.co/t/suggestions-on-invoice-extraction-llms/82516\n",
      "Initial Post: Hi there,Im planning to fine tune an Invoice Model for our use case please suggest me some of the best open source Invoice Extraction models.\n",
      "Response: I tried Donut recently . It is very light weight and convenient for the taskYou can also try the layoutLM family models , i heard that they perform well\n",
      "\n",
      "Title: Suggestions on Invoice Extraction LLMs\n",
      "Link: https://discuss.huggingface.co/t/suggestions-on-invoice-extraction-llms/82516\n",
      "Initial Post: Hi there,Im planning to fine tune an Invoice Model for our use case please suggest me some of the best open source Invoice Extraction models.\n",
      "Response: This topic was automatically closed 12 hours after the last reply. New replies are no longer allowed.\n",
      "\n",
      "Title: Best model for text-to-sql conversion\n",
      "Link: https://discuss.huggingface.co/t/best-model-for-text-to-sql-conversion/93785\n",
      "Initial Post: Which model is best for text-to-sql conversion?I am lookin for something that I can fine-tune as my understanding grows.\n",
      "Response: Maybe you can try these two models? I created this tool to help user find model and dataset based on their specific request. There are other recommendations, so maybe you can give it a try?image2578×1330 278 KB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test with a sample query\n",
    "query = \"What model is best for document information extraction?\"\n",
    "search_results = search_faiss(query)\n",
    "\n",
    "# Display results\n",
    "for result in search_results:\n",
    "    print(f\"Title: {result['title']}\\nLink: {result['link']}\\nInitial Post: {result['initial_post']}\\nResponse: {result['response']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
