{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\priks\\anaconda3\\envs\\t\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load IMDb Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "imdb_dataset = load_dataset(\"imdb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "tokenized_datasets = imdb_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\", \"label\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Texts into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = tokenizer.mask_token_id\n",
    "\n",
    "# Drop the last chunk if itâ€™s smaller than chunk_size.\n",
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "# Pad the last chunk until its length equals chunk_size.\n",
    "def group_texts_with_padding(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute total length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    \n",
    "    # If the last chunk is smaller than chunk_size, pad it\n",
    "    for key in result.keys():\n",
    "        if len(result[key][-1]) < chunk_size:\n",
    "            padding_length = chunk_size - len(result[key][-1])\n",
    "            if key == \"input_ids\":\n",
    "                # Pad input_ids with the tokenizer's pad token ID\n",
    "                result[key][-1] += [tokenizer.pad_token_id] * padding_length\n",
    "            else:\n",
    "                # Pad other keys (e.g., attention_mask) with 0\n",
    "                result[key][-1] += [0] * padding_length\n",
    "\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 76170\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 74448\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 152809\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'she wants to focus her attentions to making some sort of documentary on what the average swede thought about certain political issues such as the vietnam war and race issues in the united states. in between asking politicians and ordinary denizens of stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men. < br / > < br / > what kills me about i am curious - yellow is that 40 years ago, this was considered pornographic. really, the sex and nudity'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'she wants to focus her attentions to making some sort of documentary on what the average swede thought about certain political issues such as the vietnam war and race issues in the united states. in between asking politicians and ordinary denizens of stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men. < br / > < br / > what kills me about i am curious - yellow is that 40 years ago, this was considered pornographic. really, the sex and nudity'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][1][\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine train and unsupervised Datasets for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "training_dataset = concatenate_datasets([lm_datasets[\"train\"], lm_datasets[\"unsupervised\"]])\n",
    "evaluation_dataset = lm_datasets[\"test\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Random Masking to Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are some fluctuations in our perplexity scores with each training run, One way to eliminate this source of randomness is to apply the masking once on the whole test set, and then use the default data collator in ðŸ¤— Transformers to collect the batches during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_random_mask(batch):\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    masked_inputs = data_collator(features)\n",
    "    # Create a new \"masked\" column for each column in the dataset\n",
    "    return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dataset = evaluation_dataset.remove_columns([\"word_ids\"])\n",
    "training_dataset = training_dataset.remove_columns([\"word_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply random masking to the evaluation dataset in advance to eliminate the randomness during evaluation, which ensures consistent perplexity scores across training runs. \n",
    "\n",
    "eval_dataset = evaluation_dataset.map(\n",
    "    insert_random_mask,\n",
    "    batched=True,\n",
    "    remove_columns=evaluation_dataset.column_names,\n",
    ")\n",
    "\n",
    "\n",
    "eval_dataset = eval_dataset.rename_columns(\n",
    "    {\n",
    "        \"masked_input_ids\": \"input_ids\",\n",
    "        \"masked_attention_mask\": \"attention_mask\",\n",
    "        \"masked_labels\": \"labels\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "# Note: 'data_collator' is used here to ensure random masking for MLM tasks in every training batch\n",
    "train_dataloader = DataLoader(\n",
    "    training_dataset, \n",
    "    shuffle=True,\n",
    "    batch_size=batch_size, \n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "# Evaluation DataLoader with default collator\n",
    "# Note: 'default_data_collator' or None is used here because masking was already applied to the evaluation dataset\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=default_data_collator  # Use default behavior; no random masking during evaluation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Optimizer and Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "num_train_epochs = 10\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Initialize ðŸ¤— Accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 3578/35780 [13:02<1:49:04,  4.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss=2.2544, Perplexity=9.5294\n",
      "New best model found at epoch 0 with eval_loss: 2.2544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 7156/35780 [27:36<1:37:49,  4.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=2.1854, Perplexity=8.8946\n",
      "New best model found at epoch 1 with eval_loss: 2.1854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 10734/35780 [42:11<1:24:59,  4.91it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Loss=2.1430, Perplexity=8.5250\n",
      "New best model found at epoch 2 with eval_loss: 2.1430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14312/35780 [56:45<1:14:37,  4.79it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Loss=2.1107, Perplexity=8.2537\n",
      "New best model found at epoch 3 with eval_loss: 2.1107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 17890/35780 [1:11:20<1:01:23,  4.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Loss=2.0868, Perplexity=8.0589\n",
      "New best model found at epoch 4 with eval_loss: 2.0868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 21468/35780 [1:25:55<48:45,  4.89it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Loss=2.0646, Perplexity=7.8823\n",
      "New best model found at epoch 5 with eval_loss: 2.0646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 25046/35780 [1:40:29<36:47,  4.86it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Loss=2.0485, Perplexity=7.7563\n",
      "New best model found at epoch 6 with eval_loss: 2.0485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 28624/35780 [1:55:04<24:30,  4.87it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Loss=2.0338, Perplexity=7.6430\n",
      "New best model found at epoch 7 with eval_loss: 2.0338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 32202/35780 [2:09:39<12:10,  4.90it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Loss=2.0227, Perplexity=7.5585\n",
      "New best model found at epoch 8 with eval_loss: 2.0227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35780/35780 [2:24:13<00:00,  4.89it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Loss=2.0173, Perplexity=7.5178\n",
      "New best model found at epoch 9 with eval_loss: 2.0173\n",
      "Saving the final model...\n",
      "Final model and metrics saved to distilbert-finetuned-imdb-mlm-accelerate\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "# Define directories\n",
    "output_dir = \"distilbert-finetuned-imdb-mlm-accelerate-checkpoint\"\n",
    "final_output_dir = \"distilbert-finetuned-imdb-mlm-accelerate\"\n",
    "metrics_output_file = os.path.join(output_dir, \"metrics.json\")\n",
    "log_history_file = os.path.join(output_dir, \"log_history.json\")\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Early Stopping Parameters\n",
    "metric_for_best_model = \"eval_loss\"\n",
    "greater_is_better = False  # Lower loss is better\n",
    "patience = 3  # Number of epochs to wait for improvement\n",
    "best_metric = float(\"inf\") if not greater_is_better else -float(\"inf\")\n",
    "patience_counter = 0\n",
    "\n",
    "# Initialize tracking variables\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "all_metrics = {}  # To store metrics for each epoch\n",
    "log_history = []  # To store log history for each epoch\n",
    "\n",
    "# Training and evaluation loop\n",
    "for epoch in range(num_train_epochs):\n",
    "    # ===== Training Phase =====\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        # Forward pass and loss computation\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass and optimizer step\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # ===== Evaluation Phase =====\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        # Gather losses across all devices\n",
    "        losses.append(accelerator.gather(loss.repeat(batch_size)))\n",
    "\n",
    "    # Concatenate all losses and compute mean loss\n",
    "    losses = torch.cat(losses)\n",
    "    losses = losses[: len(eval_dataset)]  # Truncate to match dataset size\n",
    "    mean_loss = torch.mean(losses).item()\n",
    "    try:\n",
    "        perplexity = math.exp(mean_loss)\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "\n",
    "    # Log epoch metrics\n",
    "    epoch_metrics = {\n",
    "        \"epoch\": epoch,\n",
    "        \"evaluation\": {\n",
    "            \"mean_loss\": mean_loss,\n",
    "            \"perplexity\": perplexity,\n",
    "        },\n",
    "    }\n",
    "    all_metrics[f\"epoch_{epoch}\"] = epoch_metrics\n",
    "    print(f\"Epoch {epoch}: Loss={mean_loss:.4f}, Perplexity={perplexity:.4f}\")\n",
    "\n",
    "    # Save metrics and log history\n",
    "    accelerator.wait_for_everyone()\n",
    "    if accelerator.is_main_process:\n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Update global metrics file\n",
    "        with open(metrics_output_file, \"w\") as f:\n",
    "            json.dump(all_metrics, f, indent=4)\n",
    "\n",
    "        # Update log history\n",
    "        log_entry = {\n",
    "            \"epoch\": epoch,\n",
    "            \"mean_loss\": mean_loss,\n",
    "            \"perplexity\": perplexity,\n",
    "        }\n",
    "        log_history.append(log_entry)\n",
    "        with open(log_history_file, \"w\") as f:\n",
    "            json.dump(log_history, f, indent=4)\n",
    "\n",
    "    # ===== Early Stopping Logic =====\n",
    "    current_metric = mean_loss if metric_for_best_model == \"eval_loss\" else perplexity\n",
    "    if (greater_is_better and current_metric > best_metric) or (not greater_is_better and current_metric < best_metric):\n",
    "        best_metric = current_metric\n",
    "        patience_counter = 0  # Reset patience counter\n",
    "        # Save the best model\n",
    "        print(f\"New best model found at epoch {epoch} with {metric_for_best_model}: {current_metric:.4f}\")\n",
    "        best_model_dir = os.path.join(output_dir, \"best_model\")\n",
    "        accelerator.wait_for_everyone()\n",
    "        if accelerator.is_main_process:\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_model.save_pretrained(best_model_dir, save_function=accelerator.save)\n",
    "            tokenizer.save_pretrained(best_model_dir)\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement for {patience_counter} epoch(s). Best {metric_for_best_model}: {best_metric:.4f}\")\n",
    "\n",
    "    # Stop training if patience is exceeded\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping triggered after {patience} epochs of no improvement.\")\n",
    "        break\n",
    "\n",
    "# ===== Save Final Model =====\n",
    "if accelerator.is_main_process:\n",
    "    print(\"Saving the final model...\")\n",
    "    final_model_dir = os.path.join(final_output_dir)\n",
    "    os.makedirs(final_model_dir, exist_ok=True)\n",
    "\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(final_model_dir, save_function=accelerator.save)\n",
    "    tokenizer.save_pretrained(final_model_dir)\n",
    "\n",
    "    # Save final metrics and logs\n",
    "    with open(os.path.join(final_model_dir, \"metrics.json\"), \"w\") as f:\n",
    "        json.dump(all_metrics, f, indent=4)\n",
    "    with open(os.path.join(final_model_dir, \"log_history.json\"), \"w\") as f:\n",
    "        json.dump(log_history, f, indent=4)\n",
    "\n",
    "    print(f\"Final model and metrics saved to {final_model_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "mask_filler = pipeline(\n",
    "    \"fill-mask\", model=\"distilbert-finetuned-imdb-mlm-accelerate\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> this is a great movie.\n",
      ">>> this is a great film.\n",
      ">>> this is a great show.\n",
      ">>> this is a great story.\n",
      ">>> this is a great documentary.\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a great [MASK].\"\n",
    "preds = mask_filler(text)\n",
    "\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
