{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotion Classification with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets torch accelerate tqdm evaluate safetensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting environment variables for the Hugging Face cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"hf_cache\"\n",
    "os.environ['HF_EVALUATE_OFFLINE'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Hugging Face emotion dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\priks\\anaconda3\\envs\\t\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using the latest cached version of the dataset since dair-ai/emotion couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'split' at hf_cache\\datasets\\dair-ai___emotion\\split\\0.0.0\\cab853a1dbdf4c42c2b3ef2173804746df8825fe (last modified on Sun Oct 20 22:36:23 2024).\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the 'split' configuration of the emotion dataset\n",
    "raw_datasets  = load_dataset(\"dair-ai/emotion\", name=\"split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'i didnt feel humiliated', 'label': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'], id=None)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"].features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'i didnt feel humiliated', 'label': 0, 'input_ids': [101, 1045, 2134, 2102, 2514, 26608, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labels', 'input_ids', 'token_type_ids', 'attention_mask']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# train_dataloader = DataLoader(\n",
    "#     tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
    "# )\n",
    "# eval_dataloader = DataLoader(\n",
    "#     tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator\n",
    "# )\n",
    "\n",
    "# for batch in train_dataloader:\n",
    "#     break\n",
    "# {k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, AutoModelForSequenceClassification\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from hf_cache\\modules\\evaluate_modules\\metrics\\evaluate-metric--accuracy\\f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Sun Oct 20 15:39:45 2024) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.\n",
      "Using the latest cached version of the module from hf_cache\\modules\\evaluate_modules\\metrics\\evaluate-metric--precision\\4e7f439a346715f68500ce6f2be82bf3272abd3f20bdafd203a2c4f85b61dd5f (last modified on Sun Oct 20 15:48:15 2024) since it couldn't be found locally at evaluate-metric--precision, or remotely on the Hugging Face Hub.\n",
      "Using the latest cached version of the module from hf_cache\\modules\\evaluate_modules\\metrics\\evaluate-metric--recall\\e40e6e98d18ff3f210f4d0b26fa721bfaa80704b1fdf890fa551cfabf94fc185 (last modified on Sun Oct 20 15:56:45 2024) since it couldn't be found locally at evaluate-metric--recall, or remotely on the Hugging Face Hub.\n",
      "Using the latest cached version of the module from hf_cache\\modules\\evaluate_modules\\metrics\\evaluate-metric--f1\\0ca73f6cf92ef5a268320c697f7b940d1030f8471714bffdb6856c641b818974 (last modified on Sun Oct 20 16:05:16 2024) since it couldn't be found locally at evaluate-metric--f1, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy[\"accuracy\"],\n",
    "        \"precision\": precision[\"precision\"],\n",
    "        \"recall\": recall[\"recall\"],\n",
    "        \"f1\": f1[\"f1\"]\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining TrainingArguments and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\priks\\anaconda3\\envs\\t\\lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"trainer\",\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate model at the end of each epoch\n",
    "    learning_rate=2e-5,  # Initial learning rate for the AdamW optimizer\n",
    "    per_device_train_batch_size=16,  # Batch size per device during training\n",
    "    per_device_eval_batch_size=16,  # Batch size per device during evaluation\n",
    "    num_train_epochs=20,  # Number of training epochs\n",
    "    warmup_steps=500,  # Number of steps to warm up the learning rate\n",
    "                        # Reason: Gradually increases the learning rate to prevent sudden large updates that could destabilize training in early stages.\n",
    "    weight_decay=0.01,  # Strength of weight decay regularization\n",
    "                        # Reason: Prevents overfitting by penalizing large weights in the model, commonly used in fine-tuning.\n",
    "                        \n",
    "    logging_steps=100,  # Log training progress every 100 steps\n",
    "    save_strategy=\"epoch\",  # Save checkpoints at the end of each epoch\n",
    "    save_total_limit=3,  # Keep only the last 3 checkpoints to save disk space\n",
    "    load_best_model_at_end=True,  # Load the best model after training is finished based on evaluation metrics\n",
    "    metric_for_best_model=\"f1\",  # Use F1 score to determine the best model\n",
    "    greater_is_better=True,  # Higher F1 score is better\n",
    "    fp16=True,  # Use mixed precision training (FP16) for faster training and reduced memory usage on NVIDIA GPUs\n",
    "    dataloader_pin_memory=True,  # Pin data loader memory for faster GPU transfers\n",
    "    gradient_accumulation_steps=2,  # Accumulate gradients over 2 steps before updating, effectively simulating a larger batch size\n",
    "    max_grad_norm=1.0,  # Clip gradients to avoid exploding gradients, with a max norm of 1.0\n",
    "    auto_find_batch_size=True  # Automatically adjust batch size to avoid out-of-memory errors\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  # Early stop if no improvement in 3 evals\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]c:\\Users\\priks\\anaconda3\\envs\\t\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "  1%|          | 100/10000 [00:35<57:30,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7792, 'grad_norm': 4.088680744171143, 'learning_rate': 3.96e-06, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–         | 200/10000 [01:09<57:07,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5477, 'grad_norm': 10.588886260986328, 'learning_rate': 7.92e-06, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 300/10000 [01:44<56:28,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1958, 'grad_norm': 11.45164966583252, 'learning_rate': 1.1920000000000001e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 400/10000 [02:19<55:48,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8575, 'grad_norm': 4.778385162353516, 'learning_rate': 1.5920000000000003e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|â–Œ         | 500/10000 [02:54<55:12,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4513, 'grad_norm': 6.632652759552002, 'learning_rate': 1.9920000000000002e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "  5%|â–Œ         | 500/10000 [03:00<55:12,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.30288323760032654, 'eval_accuracy': 0.9085, 'eval_precision': 0.8735730308574666, 'eval_recall': 0.8961583613986344, 'eval_f1': 0.8824150697552705, 'eval_runtime': 6.2477, 'eval_samples_per_second': 320.117, 'eval_steps_per_second': 20.007, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–Œ         | 600/10000 [03:36<54:44,  2.86it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2486, 'grad_norm': 15.598343849182129, 'learning_rate': 1.979368421052632e-05, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–‹         | 700/10000 [04:11<54:04,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.215, 'grad_norm': 12.71112060546875, 'learning_rate': 1.958526315789474e-05, 'epoch': 1.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 800/10000 [04:46<53:31,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.202, 'grad_norm': 13.013236999511719, 'learning_rate': 1.9374736842105263e-05, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|â–‰         | 900/10000 [05:21<52:54,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2009, 'grad_norm': 2.7342634201049805, 'learning_rate': 1.9164210526315793e-05, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 1000/10000 [05:56<52:19,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1814, 'grad_norm': 5.7650017738342285, 'learning_rate': 1.8953684210526316e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 10%|â–ˆ         | 1000/10000 [06:02<52:19,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.16976086795330048, 'eval_accuracy': 0.933, 'eval_precision': 0.9170109656227957, 'eval_recall': 0.9073851081134209, 'eval_f1': 0.9066366033963665, 'eval_runtime': 6.2274, 'eval_samples_per_second': 321.159, 'eval_steps_per_second': 20.072, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â–ˆ         | 1100/10000 [06:38<51:40,  2.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1385, 'grad_norm': 4.712910175323486, 'learning_rate': 1.8745263157894738e-05, 'epoch': 2.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–        | 1200/10000 [07:13<51:02,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1288, 'grad_norm': 1.019859790802002, 'learning_rate': 1.8534736842105264e-05, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|â–ˆâ–Ž        | 1300/10000 [07:47<50:35,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1119, 'grad_norm': 3.136040449142456, 'learning_rate': 1.832421052631579e-05, 'epoch': 2.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|â–ˆâ–        | 1400/10000 [08:22<50:04,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1248, 'grad_norm': 5.76826810836792, 'learning_rate': 1.8113684210526317e-05, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|â–ˆâ–Œ        | 1500/10000 [08:57<49:15,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1322, 'grad_norm': 29.930660247802734, 'learning_rate': 1.7903157894736844e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 15%|â–ˆâ–Œ        | 1500/10000 [09:03<49:15,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.13484564423561096, 'eval_accuracy': 0.94, 'eval_precision': 0.9099376223702098, 'eval_recall': 0.9292266499896776, 'eval_f1': 0.9169011684421239, 'eval_runtime': 6.2066, 'eval_samples_per_second': 322.237, 'eval_steps_per_second': 20.14, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|â–ˆâ–Œ        | 1600/10000 [09:39<48:50,  2.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0859, 'grad_norm': 4.782238960266113, 'learning_rate': 1.769263157894737e-05, 'epoch': 3.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|â–ˆâ–‹        | 1700/10000 [10:14<48:14,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0975, 'grad_norm': 5.247712135314941, 'learning_rate': 1.7482105263157897e-05, 'epoch': 3.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|â–ˆâ–Š        | 1800/10000 [10:49<47:46,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0894, 'grad_norm': 1.6024552583694458, 'learning_rate': 1.727157894736842e-05, 'epoch': 3.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|â–ˆâ–‰        | 1900/10000 [11:24<47:06,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1132, 'grad_norm': 2.0167009830474854, 'learning_rate': 1.706105263157895e-05, 'epoch': 3.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 2000/10000 [11:59<46:21,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1007, 'grad_norm': 9.237066268920898, 'learning_rate': 1.6850526315789473e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 20%|â–ˆâ–ˆ        | 2000/10000 [12:05<46:21,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1501253992319107, 'eval_accuracy': 0.944, 'eval_precision': 0.9218602935700503, 'eval_recall': 0.9246084581425799, 'eval_f1': 0.9229364539093524, 'eval_runtime': 6.2275, 'eval_samples_per_second': 321.157, 'eval_steps_per_second': 20.072, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|â–ˆâ–ˆ        | 2100/10000 [12:41<45:55,  2.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.064, 'grad_norm': 4.334782600402832, 'learning_rate': 1.664e-05, 'epoch': 4.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|â–ˆâ–ˆâ–       | 2200/10000 [13:16<45:15,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0832, 'grad_norm': 36.4228630065918, 'learning_rate': 1.642947368421053e-05, 'epoch': 4.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|â–ˆâ–ˆâ–Ž       | 2300/10000 [13:51<44:46,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0778, 'grad_norm': 2.7564282417297363, 'learning_rate': 1.6218947368421053e-05, 'epoch': 4.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|â–ˆâ–ˆâ–       | 2400/10000 [14:26<44:11,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0862, 'grad_norm': 4.204072952270508, 'learning_rate': 1.6010526315789475e-05, 'epoch': 4.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–Œ       | 2500/10000 [15:00<43:20,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0816, 'grad_norm': 0.2997024655342102, 'learning_rate': 1.58e-05, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 25%|â–ˆâ–ˆâ–Œ       | 2500/10000 [15:07<43:20,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.18405097723007202, 'eval_accuracy': 0.9375, 'eval_precision': 0.9093686332011804, 'eval_recall': 0.9246117364015393, 'eval_f1': 0.9138065640627282, 'eval_runtime': 6.2128, 'eval_samples_per_second': 321.916, 'eval_steps_per_second': 20.12, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|â–ˆâ–ˆâ–Œ       | 2600/10000 [15:42<43:00,  2.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0573, 'grad_norm': 1.097046136856079, 'learning_rate': 1.5589473684210528e-05, 'epoch': 5.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|â–ˆâ–ˆâ–‹       | 2700/10000 [16:17<42:24,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.055, 'grad_norm': 0.00996150728315115, 'learning_rate': 1.5378947368421054e-05, 'epoch': 5.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|â–ˆâ–ˆâ–Š       | 2800/10000 [16:52<41:52,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0533, 'grad_norm': 5.059335708618164, 'learning_rate': 1.516842105263158e-05, 'epoch': 5.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|â–ˆâ–ˆâ–‰       | 2900/10000 [17:27<41:16,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0582, 'grad_norm': 2.047006368637085, 'learning_rate': 1.4957894736842107e-05, 'epoch': 5.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 3000/10000 [18:02<40:38,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0659, 'grad_norm': 3.163299322128296, 'learning_rate': 1.4747368421052632e-05, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 3000/10000 [18:08<40:38,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21879862248897552, 'eval_accuracy': 0.9365, 'eval_precision': 0.9139202144883766, 'eval_recall': 0.911797915149342, 'eval_f1': 0.9127307154231555, 'eval_runtime': 6.23, 'eval_samples_per_second': 321.025, 'eval_steps_per_second': 20.064, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|â–ˆâ–ˆâ–ˆ       | 3100/10000 [18:44<40:06,  2.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0407, 'grad_norm': 0.0955037772655487, 'learning_rate': 1.4536842105263159e-05, 'epoch': 6.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 3200/10000 [19:19<39:31,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0491, 'grad_norm': 8.644497871398926, 'learning_rate': 1.4326315789473685e-05, 'epoch': 6.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3300/10000 [19:54<38:58,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0445, 'grad_norm': 2.0437138080596924, 'learning_rate': 1.4115789473684212e-05, 'epoch': 6.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 3400/10000 [20:29<38:23,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0515, 'grad_norm': 0.038315534591674805, 'learning_rate': 1.3905263157894737e-05, 'epoch': 6.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 3500/10000 [21:04<37:41,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0351, 'grad_norm': 0.03849758580327034, 'learning_rate': 1.3694736842105265e-05, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 3500/10000 [21:10<37:41,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25887352228164673, 'eval_accuracy': 0.9395, 'eval_precision': 0.9323470481226465, 'eval_recall': 0.9018501516898346, 'eval_f1': 0.9154533008896871, 'eval_runtime': 6.2253, 'eval_samples_per_second': 321.269, 'eval_steps_per_second': 20.079, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 3500/10000 [21:11<39:21,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1271.509, 'train_samples_per_second': 251.669, 'train_steps_per_second': 7.865, 'train_loss': 0.25444636344909666, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3500, training_loss=0.25444636344909666, metrics={'train_runtime': 1271.509, 'train_samples_per_second': 251.669, 'train_steps_per_second': 7.865, 'total_flos': 2.9469496541184e+16, 'train_loss': 0.25444636344909666, 'epoch': 7.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the Model and Tokenizer\n",
    "\n",
    "import json\n",
    "model_output_dir = \"model\"\n",
    "\n",
    "trainer.save_model(model_output_dir)\n",
    "\n",
    "tokenizer.save_pretrained(model_output_dir)\n",
    "\n",
    "# Save the evaluation metrics as a JSON file\n",
    "metrics_output_file = model_output_dir + \"/metrics.json\"\n",
    "with open(metrics_output_file, \"w\") as f:\n",
    "    json.dump(trainer.state.log_history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dir: trainer\n",
      "overwrite_output_dir: False\n",
      "do_train: False\n",
      "do_eval: True\n",
      "do_predict: False\n",
      "eval_strategy: epoch\n",
      "prediction_loss_only: False\n",
      "per_device_train_batch_size: 16\n",
      "per_device_eval_batch_size: 16\n",
      "per_gpu_train_batch_size: None\n",
      "per_gpu_eval_batch_size: None\n",
      "gradient_accumulation_steps: 2\n",
      "eval_accumulation_steps: None\n",
      "eval_delay: 0\n",
      "torch_empty_cache_steps: None\n",
      "learning_rate: 2e-05\n",
      "weight_decay: 0.01\n",
      "adam_beta1: 0.9\n",
      "adam_beta2: 0.999\n",
      "adam_epsilon: 1e-08\n",
      "max_grad_norm: 1.0\n",
      "num_train_epochs: 20\n",
      "max_steps: -1\n",
      "lr_scheduler_type: linear\n",
      "lr_scheduler_kwargs: {}\n",
      "warmup_ratio: 0.0\n",
      "warmup_steps: 500\n",
      "log_level: passive\n",
      "log_level_replica: warning\n",
      "log_on_each_node: True\n",
      "logging_dir: trainer\\runs\\Oct20_23-26-19_Prism\n",
      "logging_strategy: steps\n",
      "logging_first_step: False\n",
      "logging_steps: 100\n",
      "logging_nan_inf_filter: True\n",
      "save_strategy: epoch\n",
      "save_steps: 500\n",
      "save_total_limit: 3\n",
      "save_safetensors: True\n",
      "save_on_each_node: False\n",
      "save_only_model: False\n",
      "restore_callback_states_from_checkpoint: False\n",
      "no_cuda: False\n",
      "use_cpu: False\n",
      "use_mps_device: False\n",
      "seed: 42\n",
      "data_seed: None\n",
      "jit_mode_eval: False\n",
      "use_ipex: False\n",
      "bf16: False\n",
      "fp16: True\n",
      "fp16_opt_level: O1\n",
      "half_precision_backend: auto\n",
      "bf16_full_eval: False\n",
      "fp16_full_eval: False\n",
      "tf32: None\n",
      "local_rank: 0\n",
      "ddp_backend: None\n",
      "tpu_num_cores: None\n",
      "tpu_metrics_debug: False\n",
      "debug: []\n",
      "dataloader_drop_last: False\n",
      "eval_steps: None\n",
      "dataloader_num_workers: 0\n",
      "dataloader_prefetch_factor: None\n",
      "past_index: -1\n",
      "run_name: trainer\n",
      "disable_tqdm: False\n",
      "remove_unused_columns: True\n",
      "label_names: None\n",
      "load_best_model_at_end: True\n",
      "metric_for_best_model: f1\n",
      "greater_is_better: True\n",
      "ignore_data_skip: False\n",
      "fsdp: []\n",
      "fsdp_min_num_params: 0\n",
      "fsdp_config: {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}\n",
      "fsdp_transformer_layer_cls_to_wrap: None\n",
      "accelerator_config: {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}\n",
      "deepspeed: None\n",
      "label_smoothing_factor: 0.0\n",
      "optim: adamw_torch\n",
      "optim_args: None\n",
      "adafactor: False\n",
      "group_by_length: False\n",
      "length_column_name: length\n",
      "report_to: ['tensorboard']\n",
      "ddp_find_unused_parameters: None\n",
      "ddp_bucket_cap_mb: None\n",
      "ddp_broadcast_buffers: None\n",
      "dataloader_pin_memory: True\n",
      "dataloader_persistent_workers: False\n",
      "skip_memory_metrics: True\n",
      "use_legacy_prediction_loop: False\n",
      "push_to_hub: False\n",
      "resume_from_checkpoint: None\n",
      "hub_model_id: None\n",
      "hub_strategy: every_save\n",
      "hub_token: <HUB_TOKEN>\n",
      "hub_private_repo: False\n",
      "hub_always_push: False\n",
      "gradient_checkpointing: False\n",
      "gradient_checkpointing_kwargs: None\n",
      "include_inputs_for_metrics: False\n",
      "eval_do_concat_batches: True\n",
      "fp16_backend: auto\n",
      "evaluation_strategy: epoch\n",
      "push_to_hub_model_id: None\n",
      "push_to_hub_organization: None\n",
      "push_to_hub_token: <PUSH_TO_HUB_TOKEN>\n",
      "mp_parameters: \n",
      "auto_find_batch_size: True\n",
      "full_determinism: False\n",
      "torchdynamo: None\n",
      "ray_scope: last\n",
      "ddp_timeout: 1800\n",
      "torch_compile: False\n",
      "torch_compile_backend: None\n",
      "torch_compile_mode: None\n",
      "dispatch_batches: None\n",
      "split_batches: None\n",
      "include_tokens_per_second: False\n",
      "include_num_input_tokens_seen: False\n",
      "neftune_noise_alpha: None\n",
      "optim_target_modules: None\n",
      "batch_eval_metrics: False\n",
      "eval_on_start: False\n",
      "use_liger_kernel: False\n",
      "eval_use_gather_object: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Load the training arguments from the bin file\n",
    "training_args = torch.load(\"model/training_args.bin\")\n",
    "\n",
    "# Convert to dictionary to view contents\n",
    "training_args_dict = training_args.to_dict()\n",
    "\n",
    "# Print the training arguments\n",
    "for key, value in training_args_dict.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training arguments saved to training_args.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Function to filter out non-serializable objects\n",
    "def custom_encoder(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: v for k, v in obj.items() if isinstance(v, (int, float, str, bool, list, dict))}\n",
    "    return str(obj)  # If it's an unsupported type, convert it to a string\n",
    "\n",
    "# Assuming `training_args` is your TrainingArguments object\n",
    "training_args_dict = vars(training_args)\n",
    "\n",
    "# Manually filter out non-serializable objects and save as JSON\n",
    "with open(\"model/training_args.json\", \"w\") as f:\n",
    "    json.dump(training_args_dict, f, default=custom_encoder, indent=4)\n",
    "\n",
    "print(\"Training arguments saved to training_args.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:15<00:00,  8.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set evaluation metrics:\n",
      "{'accuracy': 0.918, 'precision': 0.8663272444541898, 'recall': 0.8756554808732849, 'f1': 0.8705580483226839}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "model_output_dir = \"model\"\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_output_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_output_dir)\n",
    "\n",
    "# Prepare the test dataset\n",
    "test_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "# Create DataLoader for the test dataset\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True, collate_fn=data_collator)\n",
    "\n",
    "# Move model to the appropriate device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize lists to store logits and true labels\n",
    "logits_list = []\n",
    "true_labels = []\n",
    "\n",
    "# Run inference on the test dataset\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        # Move batch to the same device as the model\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Store logits and true labels\n",
    "        logits_list.append(logits.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Convert logits to a NumPy array for metric calculation\n",
    "logits_array = np.concatenate(logits_list, axis=0)\n",
    "true_labels_array = np.array(true_labels)\n",
    "\n",
    "# Compute and print the metrics using your compute_metrics function\n",
    "metrics = compute_metrics((logits_array, true_labels_array))\n",
    "\n",
    "# Print the results\n",
    "print(\"Test set evaluation metrics:\")\n",
    "print(metrics)\n",
    "\n",
    "# Optionally, save the metrics to a file\n",
    "import json\n",
    "test_metrics_output_file = model_output_dir + \"/test_metrics.json\"\n",
    "with open(test_metrics_output_file, \"w\") as f:\n",
    "    json.dump(metrics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "# Load the model configuration\n",
    "model_name = \"model\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "# Set `id2label` and `label2id` dynamically from the dataset\n",
    "config.id2label = {i: label for i, label in enumerate(raw_datasets[\"train\"].features[\"label\"].names)}\n",
    "config.label2id = {label: i for i, label in enumerate(raw_datasets[\"train\"].features[\"label\"].names)}\n",
    "\n",
    "# Save the updated configuration back to the model repository\n",
    "config.save_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/Prikshit7766/bert-base-uncased-emotion', endpoint='https://huggingface.co', repo_type='model', repo_id='Prikshit7766/bert-base-uncased-emotion')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "repo_name = \"bert-base-uncased-emotion\"\n",
    "repo_id = f\"Prikshit7766/{repo_name}\"\n",
    "token = \"\" \n",
    "api = HfApi()\n",
    "api.create_repo(repo_id=repo_id, private=False, token=token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded ./model\\config.json to Hugging Face repository Prikshit7766/bert-base-uncased-emotion\n",
      "Uploaded ./model\\metrics.json to Hugging Face repository Prikshit7766/bert-base-uncased-emotion\n",
      "Uploaded ./model\\model.safetensors to Hugging Face repository Prikshit7766/bert-base-uncased-emotion\n",
      "Uploaded ./model\\README.md to Hugging Face repository Prikshit7766/bert-base-uncased-emotion\n",
      "Uploaded ./model\\special_tokens_map.json to Hugging Face repository Prikshit7766/bert-base-uncased-emotion\n",
      "Uploaded ./model\\test_metrics.json to Hugging Face repository Prikshit7766/bert-base-uncased-emotion\n",
      "Uploaded ./model\\tokenizer.json to Hugging Face repository Prikshit7766/bert-base-uncased-emotion\n",
      "Uploaded ./model\\tokenizer_config.json to Hugging Face repository Prikshit7766/bert-base-uncased-emotion\n",
      "Uploaded ./model\\training_args.bin to Hugging Face repository Prikshit7766/bert-base-uncased-emotion\n",
      "Uploaded ./model\\training_args.json to Hugging Face repository Prikshit7766/bert-base-uncased-emotion\n",
      "Uploaded ./model\\vocab.txt to Hugging Face repository Prikshit7766/bert-base-uncased-emotion\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Initialize API and repository details\n",
    "repo_name = \"bert-base-uncased-emotion\"\n",
    "repo_id = f\"Prikshit7766/{repo_name}\"\n",
    "model_dir = \"./model\"  # Path to the model directory where all your files are located\n",
    "\n",
    "# Initialize HfApi\n",
    "api = HfApi()\n",
    "\n",
    "# Loop through all files in the model directory\n",
    "for root, dirs, files in os.walk(model_dir):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)  # Full path to the file\n",
    "        relative_path_in_repo = file  # This is the path that will be used in the repo\n",
    "        \n",
    "        # Upload the file to Hugging Face Hub\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=file_path,\n",
    "            path_in_repo=relative_path_in_repo,\n",
    "            repo_id=repo_id,\n",
    "            token=token\n",
    "        )\n",
    "        print(f\"Uploaded {file_path} to Hugging Face repository {repo_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
